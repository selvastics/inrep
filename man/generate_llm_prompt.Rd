% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llm_assistance.R
\name{generate_llm_prompt}
\alias{generate_llm_prompt}
\title{Generate LLM Customization Prompt}
\usage{
generate_llm_prompt(config = NULL, focus = "general", item_bank = NULL)
}
\arguments{
\item{config}{Study configuration from \code{\link{create_study_config}}.
If NULL, generates general context.}

\item{focus}{Focus area: "stopping_rules", "selection", "estimation", "general".}

\item{item_bank}{Optional item bank for additional context.}
}
\value{
Invisibly returns prompt text.
}
\description{
Generates a prompt with your current configuration and inrep package context.
Copy the output to an LLM (ChatGPT, Claude, etc.) to get customized R code.

\strong{Best Practice for Optimal Results:}

The LLM will provide the most accurate customization advice when one gives it 
as much context as possible about the inrep functions. Here's the recommended workflow:

\enumerate{
  \item \strong{Start}: Generate the initial prompt with this function
  \item \strong{Copy to LLM}: Paste the prompt into ChatGPT/Claude
  \item \strong{Add Function Context}: If the LLM asks about specific parameters or you 
        encounter errors, search the inrep source code for those terms
  \item \strong{Copy Relevant Code}: Add the relevant function code snippets back to your 
        conversation with the LLM
  \item \strong{Iterate}: The LLM can now give precise advice based on the actual 
        implementation details
}

Example: If one get an error about \code{min_SEM} or wants to understand \code{theta_prior},
look up those parameters in the inrep documentation or source code and share that context
with the LLM. This allows the LLM to provide accurate, implementation-specific guidance
rather than generic advice.
}
\examples{
\dontrun{
config <- create_study_config(
  name = "My Study",
  model = "GRM",
  min_items = 5,
  max_items = 20,
  min_SEM = 0.3
)
generate_llm_prompt(config, focus = "general")
}
}

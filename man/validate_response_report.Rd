% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/enhanced_reporting.R
\name{validate_response_report}
\alias{validate_response_report}
\title{Validate Response Report Consistency for Quality Assurance}
\usage{
validate_response_report(original_responses, report_data, config)
}
\arguments{
\item{original_responses}{Vector of original input responses from participants.
Should match the responses used to generate the report.}

\item{report_data}{Data frame containing the response report generated by
\code{\link{create_enhanced_response_report}}.}

\item{config}{Study configuration object created by \code{\link{create_study_config}}.
Must contain model specifications for appropriate validation logic.}
}
\value{
List containing validation results with the following components:
\describe{
  \item{consistent}{Logical indicating whether the report is consistent with input data}
  \item{original_count}{Number of original responses}
  \item{reported_count}{Number of responses in the report}
  \item{model}{IRT model used for validation}
  \item{timestamp}{Timestamp of validation execution}
  \item{details}{Additional validation details and diagnostics}
}
}
\description{
Validates that the response report accurately reflects the input data,
ensuring data integrity and consistency throughout the assessment workflow.
This function provides comprehensive quality assurance for IRT-Based Assessments.
}
\details{
This function performs comprehensive validation of response report consistency:

\strong{Validation Logic:}
\itemize{
  \item \strong{GRM Models}: Compares original ordinal responses with reported responses
  \item \strong{Binary Models}: Validates correct/incorrect scoring consistency
  \item \strong{Count Validation}: Ensures response counts match between input and report
  \item \strong{Data Integrity}: Checks for missing values and data corruption
}

\strong{Quality Assurance Features:}
\itemize{
  \item Response count verification
  \item Data type consistency checking
  \item Missing value detection
  \item Scoring logic validation
  \item Timestamp tracking for audit trails
}

\strong{Error Detection:}
\itemize{
  \item Mismatched response counts
  \item Invalid scoring transformations
  \item Missing or corrupted data
  \item Inconsistent response formats
}
}
\examples{
\dontrun{
# Example 1: Basic GRM Report Validation
library(inrep)
data(bfi_items)

# Create configuration and simulate data
config <- create_study_config(model = "GRM", language = "en")

# Original responses
original_responses <- c(3, 2, 4, 1, 5)

# Simulate CAT results
cat_result <- list(
  responses = original_responses,
  administered = c(1, 5, 12, 18, 23),
  response_times = c(2.3, 1.8, 3.2, 2.1, 2.7)
)

# Create report
report <- create_enhanced_response_report(config, cat_result, bfi_items)

# Validate report consistency
validation_result <- validate_response_report(original_responses, report, config)
print(validation_result)

# Check validation status
if (validation_result$consistent) {
  cat("[OK] Report validation PASSED\n")
} else {
  cat("[X] Report validation FAILED\n")
}

# Example 2: Binary Model Report Validation
# Create binary item bank
binary_items <- data.frame(
  Question = c("What is 2+2?", "What is 5*3?", "What is 10/2?"),
  Answer = c("4", "15", "5"),
  Option1 = c("2", "10", "3"),
  Option2 = c("3", "12", "4"),
  Option3 = c("4", "15", "5"),
  Option4 = c("5", "18", "6")
)

# Create binary configuration
binary_config <- create_study_config(model = "2PL", language = "en")

# Original responses (participant selections)
original_binary_responses <- c("4", "12", "5")  # Mix of correct and incorrect

# Simulate scoring: correct=1, incorrect=0
scored_responses <- c(1, 0, 1)  # First and third are correct

# Create CAT result with scored responses
binary_cat_result <- list(
  responses = scored_responses,
  administered = c(1, 2, 3),
  response_times = c(1.5, 2.8, 1.2)
)

# Create binary report
binary_report <- create_enhanced_response_report(binary_config, binary_cat_result, binary_items)

# Validate binary report (using scored responses for validation)
binary_validation <- validate_response_report(scored_responses, binary_report, binary_config)
print(binary_validation)

# Example 3: Validation with Errors
# Test validation with mismatched data
mismatched_responses <- c(1, 2, 3)  # Different length

error_validation <- validate_response_report(mismatched_responses, report, config)
print(error_validation)

if (!error_validation$consistent) {
  cat("Expected inconsistency detected:\n")
  cat("  Original count:", error_validation$original_count, "\n")
  cat("  Reported count:", error_validation$reported_count, "\n")
}

# Example 4: Comprehensive Validation Workflow
# Complete validation pipeline
comprehensive_validation <- function(config, item_bank, responses) {
  cat("Starting comprehensive validation workflow...\n")
  cat("==========================================\n")
  
  # Create CAT result
  cat_result <- list(
    responses = responses,
    administered = 1:length(responses),
    response_times = runif(length(responses), 1, 4)
  )
  
  # Step 1: Create report
  cat("1. Creating response report...\n")
  report <- create_enhanced_response_report(config, cat_result, item_bank)
  cat("   Report created with", nrow(report), "rows\n")
  
  # Step 2: Validate report
  cat("2. Validating report consistency...\n")
  validation <- validate_response_report(responses, report, config)
  
  # Step 3: Report results
  cat("3. Validation results:\n")
  cat("   Consistent:", validation$consistent, "\n")
  cat("   Original count:", validation$original_count, "\n")
  cat("   Reported count:", validation$reported_count, "\n")
  cat("   Model:", validation$model, "\n")
  cat("   Timestamp:", format(validation$timestamp), "\n")
  
  if (!is.null(validation$details)) {
    cat("   Additional details:\n")
    for (detail in validation$details) {
      cat("     -", detail, "\n")
    }
  }
  
  return(validation)
}

# Run comprehensive validation
grm_validation <- comprehensive_validation(config, bfi_items, original_responses)
binary_validation <- comprehensive_validation(binary_config, binary_items, scored_responses)

# Example 5: Batch Validation
# Validate multiple assessments
batch_validation <- function(assessments) {
  cat("Batch Validation Results:\n")
  cat("========================\n")
  
  results <- list()
  for (i in seq_along(assessments)) {
    assessment <- assessments[[i]]
    cat("Assessment", i, ":\n")
    
    # Create report
    cat_result <- list(
      responses = assessment$responses,
      administered = assessment$administered,
      response_times = assessment$response_times
    )
    
    report <- create_enhanced_response_report(
      assessment$config, cat_result, assessment$item_bank
    )
    
    # Validate
    validation <- validate_response_report(
      assessment$responses, report, assessment$config
    )
    
    results[[i]] <- validation
    cat("  Status:", if (validation$consistent) "PASSED" else "FAILED", "\n")
  }
  
  # Summary
  passed <- sum(sapply(results, function(x) x$consistent))
  total <- length(results)
  cat("\nSummary:", passed, "of", total, "assessments passed validation\n")
  
  return(results)
}

# Create batch assessments
assessments <- list(
  list(
    config = config,
    item_bank = bfi_items,
    responses = c(3, 2, 4, 1, 5),
    administered = c(1, 5, 12, 18, 23),
    response_times = c(2.3, 1.8, 3.2, 2.1, 2.7)
  ),
  list(
    config = binary_config,
    item_bank = binary_items,
    responses = c(1, 0, 1),
    administered = c(1, 2, 3),
    response_times = c(1.5, 2.8, 1.2)
  )
)

# Run batch validation
batch_results <- batch_validation(assessments)
}

}
\references{
Robitzsch A, Kiefer T, Wu M (2024). TAM: Test Analysis Modules. R package version 4.2-21, https://CRAN.R-project.org/package=TAMst Analysis Modules. 
R package version 3.5-19. \url{https://CRAN.R-project.org/package=TAM}
}
\seealso{
\itemize{
  \item \code{\link{create_enhanced_response_report}} for creating response reports
  \item \code{\link{validate_response_mapping}} for response mapping validation
  \item \code{\link{create_study_config}} for configuration setup
  \item \code{\link{launch_study}} for complete assessment workflow
}
}
\keyword{data-integrity}
\keyword{psychometrics}
\keyword{quality-assurance}
\keyword{validation}

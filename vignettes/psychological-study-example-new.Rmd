---
title: "Psychological Study Example"
author: "Clievins Selva"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Psychological Study Example}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

## Introduction

This vignette demonstrates a complete psychological research study using **inrep**, from study design through data analysis. We'll implement a Big Five personality assessment with adaptive testing, showcasing the full research workflow including LLM integration and professional deployment.

## Study Overview

### Research Objectives

Our example study investigates:
- Individual differences in Big Five personality traits
- Adaptive assessment efficiency compared to fixed-length tests
- Cross-cultural validity of personality measures
- Integration of modern psychometric methods with traditional personality research

### Study Design

```{r}
library(inrep)

# Define complete study configuration
personality_study <- create_study_config(
  name = "Big Five Adaptive Personality Assessment",
  
  # Psychometric model
  model = "GRM",  # Graded Response Model for polytomous data
  
  # Adaptive testing parameters
  adaptive_parameters = list(
    max_items = 30,
    min_items = 15,
    min_SEM = 0.4,
    stopping_rule = "combined",
    content_balancing = TRUE
  ),
  
  # Study population
  target_population = list(
    age_range = c(18, 65),
    languages = c("en", "de", "es"),
    recruitment = "university_community",
    expected_n = 500
  ),
  
  # Data collection features
  data_collection = list(
    estimated_duration = "15-25 minutes",
    demographic_survey = TRUE,
    feedback_provided = TRUE,
    longitudinal_tracking = FALSE
  )
)
```

## Item Bank Development

### Loading and Validating Items

```{r}
# Load the Big Five item bank
data("bfi_items", package = "inrep")

# Validate item bank compatibility
validation_results <- validate_item_bank(
  item_bank = bfi_items,
  model = "GRM",
  min_categories = 4,
  check_discrimination = TRUE
)

print(validation_results)
```

### Item Bank Structure

```{r}
# Examine item bank properties
str(bfi_items)

# Key features:
# - 60 items measuring Big Five dimensions
# - 5-point Likert scale responses
# - Content balanced across traits
# - Psychometrically validated parameters

# Check content distribution
table(bfi_items$trait)
```

### Advanced Item Analysis

```{r}
# Generate LLM-assisted item analysis
item_analysis_prompt <- generate_item_selection_optimization_prompt(
  item_bank = bfi_items,
  target_population = "university_students",
  assessment_goals = c("precision", "efficiency", "content_coverage")
)

# The generated prompt can be used with your preferred LLM to get insights on:
# - Optimal item selection strategies
# - Content balancing recommendations
# - Population-specific calibrations
```

## Study Implementation

### Basic Study Launch

```{r}
# Launch basic personality study
basic_study <- launch_study(
  study_config = personality_study,
  item_bank = bfi_items,
  
  # Interface customization
  interface_options = list(
    theme = "professional",
    progress_display = TRUE,
    item_numbering = FALSE,
    response_validation = TRUE
  ),
  
  # Data collection settings
  data_options = list(
    save_responses = TRUE,
    timestamp_responses = TRUE,
    collect_response_times = TRUE
  )
)

# Study will be available at the returned URL
print(basic_study$study_url)
```

### Advanced Study Configuration

```{r}
# Configure study with advanced features
advanced_study <- launch_study(
  study_config = personality_study,
  item_bank = bfi_items,
  
  # Advanced adaptive features
  adaptive_features = list(
    real_time_scoring = TRUE,
    dynamic_content_balancing = TRUE,
    exposure_control = TRUE,
    ability_estimation_method = "EAP"
  ),
  
  # Research-grade data collection
  research_features = list(
    participant_tracking = TRUE,
    session_management = TRUE,
    data_quality_monitoring = TRUE,
    automated_backup = TRUE
  ),
  
  # Ethical compliance
  ethics = list(
    informed_consent = TRUE,
    data_anonymization = TRUE,
    withdrawal_rights = TRUE,
    contact_information = "researcher@university.edu"
  )
)
```

### Multi-Language Deployment

```{r}
# Deploy study in multiple languages
multilingual_study <- launch_study(
  study_config = personality_study,
  item_bank = bfi_items,
  
  # Language support
  languages = list(
    primary = "en",
    additional = c("de", "es"),
    auto_detect = TRUE,
    translation_validated = TRUE
  ),
  
  # Cultural adaptations
  cultural_features = list(
    local_norms = TRUE,
    cultural_validation = TRUE,
    region_specific_items = FALSE
  )
)
```

## Professional Deployment Options

### Research Platform Deployment

```{r}
# Deploy to professional research platform
research_deployment <- launch_to_inrep_platform(
  study_config = personality_study,
  item_bank = bfi_items,
  deployment_type = "research_study",
  
  # Principal investigator information
  pi_info = list(
    name = "Dr. Personality Researcher",
    institution = "University Psychology Department",
    email = "researcher@university.edu",
    orcid = "0000-0000-0000-0000"
  ),
  
  # Study metadata
  study_metadata = list(
    title = "Adaptive Assessment of Big Five Personality Traits",
    abstract = "Investigation of adaptive testing efficiency for personality assessment",
    keywords = c("personality", "Big Five", "adaptive testing", "IRT"),
    funding = "University Research Grant",
    irb_approval = "IRB-2025-001"
  ),
  
  # Data management
  data_management = list(
    retention_period = "7_years",
    sharing_level = "metadata_only",
    embargo_period = "12_months",
    repository = "OSF"
  )
)

# Contact selva@uni-hildesheim.de for research platform access
```

### Shiny Server Deployment

```{r}
# Deploy to institutional Shiny server
shiny_deployment <- deploy_to_shiny_server(
  study_config = personality_study,
  item_bank = bfi_items,
  
  server_config = list(
    server_url = "https://shiny.university.edu",
    authentication = "institutional_sso",
    resource_allocation = "high_memory",
    backup_frequency = "daily"
  ),
  
  security_features = list(
    ssl_required = TRUE,
    access_logging = TRUE,
    rate_limiting = TRUE,
    ip_restriction = FALSE
  )
)
```

### Cloud Platform Deployment

```{r}
# Deploy to cloud platform (shinyapps.io)
cloud_deployment <- deploy_to_shinyapps_io(
  study_config = personality_study,
  item_bank = bfi_items,
  
  # Account configuration
  account_name = "university_research",
  app_name = "big_five_adaptive_study",
  
  # Resource allocation
  resources = list(
    instance_size = "large",
    max_connections = 100,
    idle_timeout = "15m"
  ),
  
  # Monitoring and analytics
  monitoring = list(
    usage_analytics = TRUE,
    performance_monitoring = TRUE,
    error_tracking = TRUE
  )
)
```

## Data Collection and Management

### Real-Time Data Monitoring

```{r}
# Monitor study progress in real-time
study_status <- monitor_study_progress(
  study_id = advanced_study$study_id,
  
  metrics = list(
    participation_rate = TRUE,
    completion_rate = TRUE,
    average_duration = TRUE,
    data_quality_indicators = TRUE
  ),
  
  alerts = list(
    low_participation = 10,  # Alert if < 10 participants per day
    high_dropout = 0.3,      # Alert if dropout > 30%
    technical_errors = 5     # Alert if > 5 errors per hour
  )
)

print(study_status)
```

### Data Export and Analysis

```{r}
# Export collected data for analysis
study_data <- export_study_data(
  study_id = advanced_study$study_id,
  
  data_format = list(
    format = "R_dataframe",
    include_metadata = TRUE,
    anonymize_ids = TRUE,
    timestamp_format = "ISO8601"
  ),
  
  analysis_ready = list(
    score_responses = TRUE,
    calculate_abilities = TRUE,
    include_se_estimates = TRUE,
    content_subscores = TRUE
  )
)

# Data structure
str(study_data)
```

## Data Analysis Examples

### Basic Descriptive Analysis

```{r}
# Load analysis packages
library(TAM)
library(ggplot2)
library(dplyr)

# Basic descriptives
summary(study_data$responses)

# Trait score distributions
study_data$scores %>%
  select(extraversion, agreeableness, conscientiousness, 
         neuroticism, openness) %>%
  pivot_longer(everything(), names_to = "trait", values_to = "score") %>%
  ggplot(aes(x = score, fill = trait)) +
  geom_density(alpha = 0.7) +
  facet_wrap(~trait) +
  theme_minimal() +
  labs(title = "Big Five Trait Score Distributions",
       x = "Standardized Score", y = "Density")
```

### Measurement Model Analysis

```{r}
# Fit measurement model using TAM
measurement_model <- TAM::tam.mml.2pl(
  resp = study_data$item_responses,
  group = study_data$demographics$language
)

# Model fit assessment
print(measurement_model)

# Item parameter estimates
item_params <- measurement_model$item_irt
print(head(item_params))

# Person ability estimates
person_abilities <- measurement_model$person
print(head(person_abilities))
```

### Adaptive Testing Efficiency Analysis

```{r}
# Compare adaptive vs. fixed-length assessment
efficiency_analysis <- analyze_adaptive_efficiency(
  study_data = study_data,
  
  comparisons = list(
    adaptive_vs_fixed = TRUE,
    length_efficiency = TRUE,
    precision_analysis = TRUE,
    content_coverage = TRUE
  ),
  
  fixed_lengths = c(15, 20, 25, 30),
  precision_targets = c(0.3, 0.4, 0.5)
)

# Create efficiency plots
plot_efficiency_results(efficiency_analysis)
```

### Cross-Cultural Analysis

```{r}
# Test measurement invariance across cultures
invariance_analysis <- test_measurement_invariance(
  data = study_data,
  grouping_variable = "language",
  
  invariance_levels = list(
    configural = TRUE,
    metric = TRUE,
    scalar = TRUE,
    strict = TRUE
  ),
  
  fit_indices = c("CFI", "RMSEA", "SRMR"),
  significance_level = 0.01
)

print(invariance_analysis)
```

## Advanced Features

### Machine Learning Integration

```{r}
# Integrate machine learning for enhanced predictions
ml_analysis <- integrate_machine_learning(
  study_data = study_data,
  
  models = list(
    random_forest = TRUE,
    neural_network = TRUE,
    svm = TRUE
  ),
  
  prediction_targets = list(
    academic_performance = "gpa",
    job_satisfaction = "job_rating",
    well_being = "life_satisfaction"
  ),
  
  cross_validation = list(
    method = "k_fold",
    k = 10,
    repeats = 5
  )
)
```

### Longitudinal Analysis Setup

```{r}
# Prepare for longitudinal follow-up
longitudinal_setup <- setup_longitudinal_study(
  baseline_study = advanced_study,
  
  follow_up_schedule = list(
    wave_2 = "6_months",
    wave_3 = "12_months",
    wave_4 = "24_months"
  ),
  
  measurement_design = list(
    item_overlap = 0.7,  # 70% item overlap between waves
    new_items = 0.3,     # 30% new items per wave
    adaptive_linking = TRUE
  ),
  
  participant_retention = list(
    reminder_system = TRUE,
    incentive_structure = "graduated",
    contact_tracking = TRUE
  )
)
```

## Quality Assurance

### Data Quality Assessment

```{r}
# Comprehensive data quality checks
quality_report <- assess_data_quality(
  study_data = study_data,
  
  quality_indicators = list(
    response_patterns = TRUE,
    timing_analysis = TRUE,
    careless_responding = TRUE,
    missing_data_patterns = TRUE
  ),
  
  flagging_criteria = list(
    too_fast = "< 2_seconds_per_item",
    too_slow = "> 5_minutes_per_item",
    straight_lining = "> 80_percent_same_response",
    missing_threshold = "> 20_percent_missing"
  )
)

print(quality_report)
```

### Psychometric Validation

```{r}
# Validate psychometric properties
psychometric_validation <- validate_psychometric_properties(
  study_data = study_data,
  
  reliability_analysis = list(
    cronbach_alpha = TRUE,
    omega_total = TRUE,
    test_retest = TRUE  # If available
  ),
  
  validity_analysis = list(
    construct_validity = TRUE,
    convergent_validity = TRUE,
    discriminant_validity = TRUE,
    criterion_validity = TRUE
  ),
  
  factor_analysis = list(
    confirmatory = TRUE,
    exploratory = TRUE,
    bifactor = TRUE
  )
)
```

## Reporting and Dissemination

### Automated Report Generation

```{r}
# Generate comprehensive study report
study_report <- generate_study_report(
  study_data = study_data,
  analyses = list(efficiency_analysis, invariance_analysis),
  
  report_sections = list(
    executive_summary = TRUE,
    methodology = TRUE,
    participants = TRUE,
    psychometric_properties = TRUE,
    main_findings = TRUE,
    implications = TRUE,
    limitations = TRUE,
    recommendations = TRUE
  ),
  
  output_format = "pdf",
  include_plots = TRUE,
  technical_appendix = TRUE
)
```

### Open Science Documentation

```{r}
# Prepare open science documentation
open_science_package <- create_open_science_package(
  study_data = study_data,
  
  components = list(
    preregistration = "osf_link",
    analysis_code = TRUE,
    data_dictionary = TRUE,
    supplementary_materials = TRUE
  ),
  
  sharing_level = list(
    data = "summarized",
    code = "full",
    materials = "full"
  ),
  
  documentation = list(
    methodology_protocol = TRUE,
    analysis_pipeline = TRUE,
    computational_environment = TRUE
  )
)
```

## Study Variants and Extensions

### Clinical Application

```{r}
# Adapt for clinical assessment
clinical_variant <- adapt_for_clinical_use(
  base_study = personality_study,
  
  clinical_features = list(
    diagnostic_relevance = TRUE,
    severity_assessment = TRUE,
    treatment_planning = TRUE,
    outcome_measurement = TRUE
  ),
  
  safety_features = list(
    crisis_detection = TRUE,
    referral_pathways = TRUE,
    professional_supervision = TRUE
  )
)
```

### Educational Application

```{r}
# Adapt for educational settings
educational_variant <- adapt_for_education(
  base_study = personality_study,
  
  educational_features = list(
    academic_relevance = TRUE,
    developmental_appropriateness = TRUE,
    classroom_integration = TRUE,
    teacher_training = TRUE
  ),
  
  learning_analytics = list(
    learning_style_prediction = TRUE,
    intervention_recommendation = TRUE,
    progress_tracking = TRUE
  )
)
```

## Conclusion

This comprehensive example demonstrates the full research workflow using **inrep**, from initial study design through advanced analysis and dissemination. Key benefits include:

- **Methodological Rigor**: Built on validated psychometric foundations (TAM)
- **Efficiency**: Adaptive testing reduces participant burden while maintaining precision
- **Flexibility**: Supports diverse research designs and populations
- **Integration**: Compatible with existing research infrastructure and analysis tools
- **Innovation**: LLM integration for enhanced research capabilities
- **Scalability**: From small pilot studies to large-scale data collection

The **inrep** package enables researchers to implement cutting-edge adaptive assessment methods while maintaining the highest standards of psychometric quality and research ethics.

For specific questions about implementing similar studies, contact selva@uni-hildesheim.de or refer to the package documentation.
